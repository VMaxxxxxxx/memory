1. 介绍

   > 本项目针对`Linux`下的日志文件友好访问需求，采用主从多线程的`Reactor`模式和`IO`多路复用技术，实现了一个轻量级的高性能`HTTP`服务器。
   >
   > 核心模块：事件循环、调度器、线程池、`HTTP`请求解析与响应生成、缓冲区、`TCP`连接与服务器管理。
   >
   > 核心特性：`Reactor`事件驱动架构（事件循环`+`事件调度）、多线程`one loop per thread`（主线程、工作线程、线程池的配合）、`IO`多路复用（采用`epoll`，但也实现了`select`、`poll`）、`HTTP`请求解析与响应生成、高效缓冲区管理（动态扩容）、高并发性能（`QPS`可达`1000`，延迟控制在`50ms`）

2. `Reactor`模式的核心思想

   > 本质是**`事件驱动、非阻塞IO`**，主线程只负责监听事件（如`socket`可读可写），一旦有事件发生就分发给对应的处理器（回调函数）处理；可以高效处理大量并发的连接，避免每个连接都使用一个线程或进程

3. 事件驱动框架的事件流转流程

   > - 新连接到来：主线程通过`Dispatcher`监听`listen fd`的可读事件，`accept`新连接。
   > - 连接分发：主线程将新连接分配给某个工作线程的事件循环。
   > - 事件注册：工作线程将新连接的`fd`注册到自己的`Dispatcher`，监听可读/可写等事件。
   > - 事件检测：`Dispatcher`（如`epoll`）检测到`fd`有事件发生，通知事件循环。
   > - 事件分发：事件循环遍历活跃`fd`，调用对应`Channel`的回调函数处理事件。
   > - 业务处理：回调函数完成协议解析、数据收发、响应生成等业务逻辑。

4. 事件循环**`EventLoop`**

   > 作为`Reactor`的核心，在`while`中调用`epoll`、`select`、`poll`等系统调用，等待`IO`事件发生
   >
   > 当事件发生时，事件循环会分发事件给注册的回调处理函数

5. 事件分发器`Dispatcher`

   > 负责将事件（某`fd`可读可写）分发给对应的处理逻辑（读取请求等）

6. 线程池`ThreadPool`

   > 通过**预先创建**多个工作线程，避免频繁创建/销毁线程带来的开销，每个工作线程都运行自己的事件循环`EventLoop`，负责处理分配给他的连接上的事件

7. `one loop per thread`模型

   > 每个工作线程有独立的事件循环（**一个线程一个事件循环**），**主线程**只负责监听新连接，接收到新连接后将其分配给工作线程的事件循环，由该线程负责后续的`IO`和事件处理（读写、协议解析、响应），避免了多线程竞争同一个事件循环的锁，提高了多核利用率和并发处理能力

8. 如何区分线程类型

   > 线程池结构体里有主事件循环，能找到对应的线程`ID`，其他工作线程比较`pthread_self`进行判断
   >
   > 线程池管理所有的工作线程，主线程和工作线程的职责和入口函数不同

9. 并发性能

   > `QPS`每秒请求数、并发连接数、延迟，这个受硬件、操作系统、网络影响，单机环境下的`Reactor`线程池模型，可以支持成千上万的并发连接
   >
   > 经过`wrk`测试，启动`4`线程维护`100`个并发连接，当持续向目标服务器发送`HTTP`请求，然后收集响应事件、成功、失败等数据，统计出的`QPS`可达`1000+`，平均延迟`50ms`左右，已经远超设计初衷

10. 请求解析模块

   > 能够从`TCP`流中解析出`HTTP`请求行、请求头、请求体（当然针对日志浏览的请求，没有请求体），而且我们也只是实现了`GET`请求，能够正确提取出方法、`URL`（浏览器期待的文件或目录）、版本、头部

11. 响应生成模块

    > 根据从请求中解析出的`HTTP`的请求数据细节，动态生成并拼接出符合`HTTP`协议的响应，包括状态行、响应头、空行、响应体（在请求文件的时候，填充为文件内容）

12. 缓冲区动态扩容

    > 构造了一个`buffer`缓冲区模块，在应对大文件或长请求的时候支持自动扩容

13. 缓冲区的作用

    > 网络数据基于流，数据可能分次到达或发送，用缓冲区临时存储从`socket`中读取的数据，以及待发送的数据，尽量保证数据的完整性和高效处理
    >
    > 发送的时候，其实是规避了`TCP`粘包的问题

14. 静态日志文件服务

    > 在返回目录结构时，一开始以文件后缀进行了一遍过滤（`.log`、`.txt`、`.log.txt`），我后续取消了这个限制来使这项目更符合通用文件服务

15. `IO`多路复用的作用

    > 传统阻塞`IO`模型每个连接一个线程，资源消耗大，难以支撑高并发，`IO`多路复用允许单线程同时监听多个`socket`的`IO`事件

16. `Dispatcher`负责什么

    > 负责注册、监听、分发`socket`事件（新连接到达、数据可读可写）

17. `epoll`的处理流程

    > 事件注册：当新的`socket`连接建立之后，通过`epoll_ctl`注册到`epoll`实例，监听其读写事件
    >
    > 事件循环：主线程或工作线程调用`epoll_wait`等待事件发生
    >
    > 事件分发：一旦有事件发生，`epoll`返回活跃的`fd`列表，事件循环遍历这些`fd`，然后调用对应的回调函数进行处理（读取`HTTP`请求、发送响应）
    >
    > 高并发处理：由于`epoll`高效，单线程即可管理成千上万个连接，再加上线程池模型，每个线程都能独立高效处理大量连接
    >
    > 封装了三种`IO`多路复用机制（`select`、`poll`、`epoll`），通过统一的`Dispatcher`接口注册和监听`socket`事件。事件循环采用`epoll_wait`高效等待活跃的`fd`，将事件分发给对应的回调函数，实现了单线程高效管理成千上万个并发连接。
    >
    > 主线程：只负责`accept`新连接，不处理具体业务。
    > 工作线程：每个线程有自己的事件循环（`event loop`），用`epoll`等技术管理和处理分配给它的所有连接，当某个连接有事件（如可读/可写）时，`epoll`会通知线程，线程再去处理对应的连接和数据。。
    > 每个工作线程内部是“单线程高并发”，整个服务器是“多线程+每线程高并发”。

18. `Reactor`模型，传统阻塞`IO`服务模型

    > ![img](https://pic3.zhimg.com/v2-f9d7285211b70601aa33a5da7ab04b08_r.jpg)
    >
    > - 网络请求经过：服务器网卡、内核、连接建立、数据读取、业务处理、数据写回；其中`accept`、`read`、`write`都需要操作系统内核提供的系统调用，由内核与网卡进行数据交互，而这些`IO`调用消耗高，因此可对每个连接使用独立的一个线程处理，因此`N`个连接需要`N`个线程资源。
    >
    > - **阻塞`IO`**：用户线程一直阻塞等待网卡数据准备就绪，大部分时间都在等待`IO`事件就绪
    >
    > - **非阻塞`IO`**：如果数据未就绪，用户线程直接返回，应用层轮训读取、查询，直到成功读取数据
    >
    > - **`IO`多路复用（感官）**：通过一个阻塞的系统调用（如 `select`、`poll`、`epoll`），**委托内核同时监测多个文件描述符的状态（核心）**（本质是检测`fd`对应的**缓冲区**的可读、可写、异常等）。一旦有文件描述符就绪，系统调用返回，应用程序即可针对这些就绪的文件描述符进行通信操作。这样在单线程或单进程的场景下，也能实现对多个客户端的并发处理。
    >
    > - **事件驱动（大脑）**：等待事件发生（事件循环`eventLoop`），检测到事件后，调用对应（事件分发`dispatcher`）的处理函数（回调函数、`handler`）
    >
    > - **`Reactor`模型**：`IO`复用`+`线程池：通过一个或多个输入同时传递给服务处理器的服务请求的事件驱动处理模式，**核心是事件驱动**，三种形式（单线程模型、多线程模型、主从多线程模型）
    >
    >   - *单线程模型*：一个`reactor`线程用`select`等监听所有事件，就绪后，直接在同一个线程中调用对应的处理器，`IO`操作和业务逻辑都在一个线程，任何耗时操作会阻塞整个服务器
    >
    >   - <img src="https://pic2.zhimg.com/v2-dae5287666bb6dd3d361d4f673a35e0f_r.jpg" alt="img" style="zoom: 33%;" />
    >
    >   - *多线程模型*：一个`reactor`线程负责`IO`多路复用，当某连接有读写事件，`Reactor`负责分发事件，然后交给线程池的工作线程处理业务逻辑，`reactor`仍然是单线程，若连接数多，仍会阻塞
    >
    >   - <img src="https://picx.zhimg.com/v2-1161fb3d6420a5b32b06321ba288aabb_r.jpg" alt="img" style="zoom:28%;" />
    >
    >   - *主从多线程模型*：主`reactor`只负责监听连接请求，把新连接分配给某个从`reactor`，从`reactor`负责监听自家管理的连接的读写事件，并分发给对应的`Handler`，而工作线程由从`reactor`调度来处理具体业务逻辑，主从分离充分利用多核`CPU`，连接分布在多个`Reactor`
    >
    >   - <img src="https://ask.qcloudimg.com/http-save/8495412/16izwrc9gd.jpeg" alt="img" style="zoom:50%;" />
    >
    >   - ```shell
    >     # MainReactor对象通过select监控连接建立事件，收到后，通过accept接收，处理建立连接事件，然后MainReactor分配子线程给SubReactor处理
    >     # SubReactor将连接加入连接队列进行监听，创建Handler处理各种连接事件
    >     # 有新的事件发生，SubReactor会抵用连接对应的Handler进行响应
    >     # Handler通过read读数据，分发给线程池Worker进行业务处理
    >     # 线程池分配独立的线程完成真正的业务处理，将响应结果发给Handler
    >     # Handler收到响应，再send返回给Client
    >     ```
    >
    > - 每个连接都需要独立的线程完成处理完整操作，当并发量比较大，需要创建大量线程；若没有线程可读，当前线程就阻塞在`read`上；都是资源浪费
    >
    > - 解决方案：
    >
    >   1. 基于`IO`复用模型：每个连接共用一个阻塞对象，应用程序只需要在一个阻塞对象上等待，无需阻塞等待所有连接。当某条连接有数据可处理，操作系统通知应用程序，线程从阻塞状态返回
    >
    >   1. 基于线程池复用线程资源：不必再为每个连接创建线程，将连接完成后的业务处理分配给线程进行处理，一个线程可以处理多个连接的业务

19. `IO`多路复用方式的实现细节

    > 1. 共通点
    >
    >    > 都是用一个系统调用来监听多个文件描述符
    >    >
    >    > 委托内核监控`fd`的状态
    >    >
    >    > 当事件发生时，返回给用户态，由用户态去操作就绪的`fd`
    >    >
    >    > 主要区别在于**内核如何管理和返回就绪的`fd`**
    >
    > 2. `select`
    >
    >    > - 原理：`select`通过`fd_set`结构体（用户态传入的**位图数组**）管理所有监听的`fd`，每次调用都要把所有`fd`集合从用户态**拷贝**到内核态（内核遍历，逐个检查状态，然后返回），用户态需要逐个扫描中找出就绪的`fd`，效率低（内存拷贝开销大），`fd`数量也有限（`1024`）
    >    > - 实现流程：
    >    >   1. 初始化`fd_set`（`FD_ZERO`），将所有需要监听的`fd`加入集合（`FD_SET`）
    >    >   2. 循环调用`select(maxfd+1, &readfds, &writefds, NULL, timeout)`等待事件
    >    >   3. 返回后遍历`fd`集合，判断哪些`fd`有事件发生，分发给对应的回调处理
    >    >   4. 事件处理完毕后，继续下一轮循环
    >
    > 3. `poll`
    >
    >    > - 原理：`poll`用一个`pollfd`**结构体数组**（包含`fd`、关心的事件类型）管理所有监听的`fd`，没有`fd`数量限制，但同样内核和用户态每次都要遍历整个数组，效率一般
    >    > - 实现流程：
    >    >   1. 用`pollfd`结构体数组保存所有监听的`fd`及事件类型
    >    >   2. 调用`poll(pollfds, nfds, timeout)`等待事件
    >    >   3. 返回后遍历`pollfd`数组，检查**`revents`字段**，找出有事件的`fd`，分发处理
    >    >   4. 事件处理完毕，更新`pollfd`数组，进入下一轮循环
    >
    > 4. `epoll`
    >
    >    > - 原理：`Linux`高效的`IO`多路复用机制，分两个阶段（注册（`epoll_ctl`把要监听的`fd`注册到内核的**红黑树**中）、等待（`epoll_wait`，内核基于事件通知的回调机制，在`fd`状态变化时，通过**就绪队列**（链表）返回）），因此**支持事件驱动**，内核直接通知的是活跃的`fd`，无需遍历所有的`fd`,支持**水平**（`LT`，只要`fd`就绪，每次`epoll_wait`都会返回）、**边缘**触发（`ET`，只有状态变化时才返回，需要非阻塞`IO`+循环读写）
    >    > - 实现流程：
    >    >   1. 创建`epoll`实例（`epoll_create`），获得`epollfd`
    >    >   2. 用`epoll_ctl`注册、修改、删除需要监听的`fd`及其事件类型
    >    >   3. 调用`epoll_wait(epollfd, events, maxevents, timeout)`等待事件
    >    >   4. 返回后遍历`events`数组，直接获得有事件的`fd`，分发给回调处理
    >    >   5. 事件处理完毕后，继续下一轮循环

20. 一个工作线程、一个事件循环、多个连接的关系

    > - 一个工作线程对应一个事件循环，但一个事件循环可以同时管理和处理多个连接
    > - 事件循环内部通过`IO`多路复用机制，监听和管理所有分配给他的所有`socket fd`

21. `Buffer`中读`socket`的数据，`readv`

    > - 使用分散读的策略，一次性把数据读取到多个不连续的缓冲区（`iovec`数组）中；
    >
    > - 如果数据量小，数据全部落在主`buffer`中；数据量大，主缓冲区写满后，剩下的数据直接落到临时缓冲区（`40KB = 40960bytes`），再`append`到主缓冲区（实现了自动扩容）
    > - 避免多次`read`和多次内存拷贝

22. 常用的一些函数

    > ```c
    > // 把 s 指向的内存区域的前 n 个字节设置为指 c，按字节写
    > void *memset(void *s, int c, size_t n);
    > // 把 src 开始的 n 个字节拷贝到 dest
    > void *memcpy(void *dest, const void *src, size_t n);
    > // 和 memcopy 类似，但支持内存区域重叠
    > void *memmove(void *dest, const void *src, size_t n);
    > // 比较两个内存区域的前 n 个字节
    > int memcmp(const void *s1, const void *s2, size_t n);
    > // 在内存卡 s 的前 n 个字节中查找指 c
    > void *memchr(const void *s, int c, size_t n);
    > // 在一块内存 haystack 中查找子串 needle
    > void *memmem(const void *haystack, size_t haystacklen, const void *needle, size_t needlelen);
    > ```

23. 

    